{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from course_small_datasets import training, loader, model\n",
    "from course_small_datasets import TestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'course_small_datasets/starter_code/part2-synthetic/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_data + 'loan_continuous.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(df: pd.DataFrame):\n",
    "    col_summary = lambda col, fn, df: getattr(df[col], fn)()\n",
    "    cols_summary = lambda fn, df: [col_summary(col, fn, df) for col in df.columns]\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "        'col name': df.columns,\n",
    "        'dtype': list(df.dtypes),\n",
    "        'min': cols_summary('min', df),\n",
    "        'max': cols_summary('max', df),\n",
    "        'mean': cols_summary('mean', df),\n",
    "        'std': cols_summary('std', df),\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col name</th>\n",
       "      <th>dtype</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Loan Amount</td>\n",
       "      <td>int64</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>3.500000e+04</td>\n",
       "      <td>16848.902776</td>\n",
       "      <td>8367.865726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Funded Amount</td>\n",
       "      <td>int64</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>3.499900e+04</td>\n",
       "      <td>15770.599114</td>\n",
       "      <td>8150.992662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Funded Amount Investor</td>\n",
       "      <td>float64</td>\n",
       "      <td>1114.590204</td>\n",
       "      <td>3.499975e+04</td>\n",
       "      <td>14621.799323</td>\n",
       "      <td>6785.345170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Term</td>\n",
       "      <td>int64</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>5.900000e+01</td>\n",
       "      <td>58.173814</td>\n",
       "      <td>3.327441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home Ownership</td>\n",
       "      <td>float64</td>\n",
       "      <td>14573.537170</td>\n",
       "      <td>4.065615e+05</td>\n",
       "      <td>80541.502522</td>\n",
       "      <td>45029.120366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Debit to Income</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.675299</td>\n",
       "      <td>3.962986e+01</td>\n",
       "      <td>23.299241</td>\n",
       "      <td>8.451824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delinquency - two years</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.327127</td>\n",
       "      <td>0.800888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Inquires - six months</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.145754</td>\n",
       "      <td>0.473291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Open Account</td>\n",
       "      <td>int64</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>14.266561</td>\n",
       "      <td>6.225060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Public Record</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.081437</td>\n",
       "      <td>0.346606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Revolving Balance</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.169330e+05</td>\n",
       "      <td>7699.342425</td>\n",
       "      <td>7836.148190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Revolving Utilities</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>1.008800e+02</td>\n",
       "      <td>52.889443</td>\n",
       "      <td>22.539450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Accounts</td>\n",
       "      <td>int64</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.200000e+01</td>\n",
       "      <td>18.627929</td>\n",
       "      <td>8.319246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Total Received Interest</td>\n",
       "      <td>float64</td>\n",
       "      <td>4.736746</td>\n",
       "      <td>1.430137e+04</td>\n",
       "      <td>2068.992542</td>\n",
       "      <td>2221.918745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Total Received Late Fee</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.261888e+01</td>\n",
       "      <td>1.143969</td>\n",
       "      <td>5.244365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Recoveries</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>4.354467e+03</td>\n",
       "      <td>59.691578</td>\n",
       "      <td>357.026346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Collection Recovery Fee</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.668330e+02</td>\n",
       "      <td>1.125141</td>\n",
       "      <td>3.489885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Collection 12 months Medical</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.144385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Last week Pay</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.610000e+02</td>\n",
       "      <td>71.163260</td>\n",
       "      <td>43.315845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Accounts Delinquent</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Total Collection Amount</td>\n",
       "      <td>int64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.642100e+04</td>\n",
       "      <td>146.467990</td>\n",
       "      <td>744.382233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Total Current Balance</td>\n",
       "      <td>int64</td>\n",
       "      <td>617.000000</td>\n",
       "      <td>1.177412e+06</td>\n",
       "      <td>159573.933638</td>\n",
       "      <td>139033.245565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Total Revolving Credit Limit</td>\n",
       "      <td>int64</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2.011690e+05</td>\n",
       "      <td>23123.005544</td>\n",
       "      <td>20916.699999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Loan Status</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.289747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        col name    dtype           min           max  \\\n",
       "0                    Loan Amount    int64   1014.000000  3.500000e+04   \n",
       "1                  Funded Amount    int64   1014.000000  3.499900e+04   \n",
       "2         Funded Amount Investor  float64   1114.590204  3.499975e+04   \n",
       "3                           Term    int64     36.000000  5.900000e+01   \n",
       "4                 Home Ownership  float64  14573.537170  4.065615e+05   \n",
       "5                Debit to Income  float64      0.675299  3.962986e+01   \n",
       "6        Delinquency - two years    int64      0.000000  8.000000e+00   \n",
       "7          Inquires - six months    int64      0.000000  5.000000e+00   \n",
       "8                   Open Account    int64      2.000000  3.700000e+01   \n",
       "9                  Public Record    int64      0.000000  4.000000e+00   \n",
       "10             Revolving Balance    int64      0.000000  1.169330e+05   \n",
       "11           Revolving Utilities  float64      0.005172  1.008800e+02   \n",
       "12                Total Accounts    int64      4.000000  7.200000e+01   \n",
       "13       Total Received Interest  float64      4.736746  1.430137e+04   \n",
       "14       Total Received Late Fee  float64      0.000003  4.261888e+01   \n",
       "15                    Recoveries  float64      0.000036  4.354467e+03   \n",
       "16       Collection Recovery Fee  float64      0.000036  1.668330e+02   \n",
       "17  Collection 12 months Medical    int64      0.000000  1.000000e+00   \n",
       "18                 Last week Pay    int64      0.000000  1.610000e+02   \n",
       "19           Accounts Delinquent    int64      0.000000  0.000000e+00   \n",
       "20       Total Collection Amount    int64      1.000000  1.642100e+04   \n",
       "21         Total Current Balance    int64    617.000000  1.177412e+06   \n",
       "22  Total Revolving Credit Limit    int64   1000.000000  2.011690e+05   \n",
       "23                   Loan Status    int64      0.000000  1.000000e+00   \n",
       "\n",
       "             mean            std  \n",
       "0    16848.902776    8367.865726  \n",
       "1    15770.599114    8150.992662  \n",
       "2    14621.799323    6785.345170  \n",
       "3       58.173814       3.327441  \n",
       "4    80541.502522   45029.120366  \n",
       "5       23.299241       8.451824  \n",
       "6        0.327127       0.800888  \n",
       "7        0.145754       0.473291  \n",
       "8       14.266561       6.225060  \n",
       "9        0.081437       0.346606  \n",
       "10    7699.342425    7836.148190  \n",
       "11      52.889443      22.539450  \n",
       "12      18.627929       8.319246  \n",
       "13    2068.992542    2221.918745  \n",
       "14       1.143969       5.244365  \n",
       "15      59.691578     357.026346  \n",
       "16       1.125141       3.489885  \n",
       "17       0.021301       0.144385  \n",
       "18      71.163260      43.315845  \n",
       "19       0.000000       0.000000  \n",
       "20     146.467990     744.382233  \n",
       "21  159573.933638  139033.245565  \n",
       "22   23123.005544   20916.699999  \n",
       "23       0.092510       0.289747  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low_count = df[df['Loan Status'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = loader.DataBuilder(df_low_count)\n",
    "testset = loader.DataBuilder(df_low_count, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size) \n",
    "testloader = DataLoader(testset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (5, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "Results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95     61222\n",
      "           1       0.00      0.00      0.00      6241\n",
      "\n",
      "    accuracy                           0.91     67463\n",
      "   macro avg       0.45      0.50      0.48     67463\n",
      "weighted avg       0.82      0.91      0.86     67463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "TestModel.test_model('course_small_datasets/starter_code/part2-synthetic/data/loan_continuous.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both **recall** and **f1-score** for `class 1` are zero, which very likely stem from the class imbalance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_params = {'D_in': trainset.x.shape[1], 'hidden_dims': [50, 12], 'latent_dim': 3}\n",
    "deep_params = {'D_in': trainset.x.shape[1], 'hidden_dims': [64, 32, 16], 'latent_dim': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a wrapper for the training. For evaluation we need to reloade the model from the returned path from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network_params, save_path, train_loader, test_loader, device):\n",
    "    vae = model.VAE(**network_params).to(device)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "    loss_fn = model.MSE_KLD()\n",
    "    path_best_model = training.train(3000, vae, optimizer, loss_fn, train_loader, test_loader,\n",
    "                                     save_every=-1, print_every=200, save_path=save_path,\n",
    "                                     grace_for_overfit=500,\n",
    "                                     patience_for_overfit=10,\n",
    "                                     relative_val_loss_threshold_for_overfit=0.02,\n",
    "                                     device=device)\n",
    "    return path_best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shallow network **without** residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 200 Average training loss 15.85\n",
      "====> Epoch: 200 Average validation loss 15.50\n",
      "====> Epoch: 400 Average training loss 15.38\n",
      "====> Epoch: 400 Average validation loss 15.07\n",
      "====> Epoch: 600 Average training loss 15.12\n",
      "====> Epoch: 600 Average validation loss 15.02\n",
      "====> Epoch: 800 Average training loss 14.99\n",
      "====> Epoch: 800 Average validation loss 15.06\n",
      "====> Epoch: 1000 Average training loss 14.90\n",
      "====> Epoch: 1000 Average validation loss 15.07\n",
      "====> Epoch: 1200 Average training loss 14.86\n",
      "====> Epoch: 1200 Average validation loss 15.05\n",
      "====> Epoch: 1400 Average training loss 14.80\n",
      "====> Epoch: 1400 Average validation loss 15.06\n",
      "====> Epoch: 1600 Average training loss 14.76\n",
      "====> Epoch: 1600 Average validation loss 15.13\n",
      "====> Epoch: 1800 Average training loss 14.75\n",
      "====> Epoch: 1800 Average validation loss 15.11\n",
      "====> Epoch: 2000 Average training loss 14.67\n",
      "====> Epoch: 2000 Average validation loss 15.20\n",
      "====> Epoch: 2200 Average training loss 14.67\n",
      "====> Epoch: 2200 Average validation loss 15.17\n",
      "====> Epoch: 2400 Average training loss 14.58\n",
      "====> Epoch: 2400 Average validation loss 15.20\n",
      "====> Epoch: 2600 Average training loss 14.59\n",
      "====> Epoch: 2600 Average validation loss 15.18\n",
      "====> Epoch: 2800 Average training loss 14.59\n",
      "====> Epoch: 2800 Average validation loss 15.14\n",
      "====> Epoch: 3000 Average training loss 14.57\n",
      "====> Epoch: 3000 Average validation loss 15.25\n"
     ]
    }
   ],
   "source": [
    "best_model_path_shallow = train(\n",
    "    {**shallow_params, 'use_shortcut': False},\n",
    "    'synthetic_data_shallow_vae',\n",
    "    trainloader, testloader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shallow network with residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 200 Average training loss 16.22\n",
      "====> Epoch: 200 Average validation loss 16.04\n",
      "====> Epoch: 400 Average training loss 15.72\n",
      "====> Epoch: 400 Average validation loss 15.71\n",
      "====> Epoch: 600 Average training loss 15.10\n",
      "====> Epoch: 600 Average validation loss 15.21\n",
      "====> Epoch: 800 Average training loss 14.96\n",
      "====> Epoch: 800 Average validation loss 15.10\n",
      "====> Epoch: 1000 Average training loss 14.83\n",
      "====> Epoch: 1000 Average validation loss 15.10\n",
      "====> Epoch: 1200 Average training loss 14.77\n",
      "====> Epoch: 1200 Average validation loss 15.16\n",
      "====> Epoch: 1400 Average training loss 14.76\n",
      "====> Epoch: 1400 Average validation loss 15.12\n",
      "====> Epoch: 1600 Average training loss 14.74\n",
      "====> Epoch: 1600 Average validation loss 15.22\n",
      "====> Epoch: 1800 Average training loss 14.62\n",
      "====> Epoch: 1800 Average validation loss 15.16\n",
      "====> Epoch: 2000 Average training loss 14.61\n",
      "====> Epoch: 2000 Average validation loss 15.13\n",
      "====> Epoch: 2200 Average training loss 14.56\n",
      "====> Epoch: 2200 Average validation loss 15.16\n",
      "====> Epoch: 2400 Average training loss 14.55\n",
      "====> Epoch: 2400 Average validation loss 15.13\n",
      "====> Epoch: 2600 Average training loss 14.53\n",
      "====> Epoch: 2600 Average validation loss 15.22\n",
      "====> Epoch: 2800 Average training loss 14.51\n",
      "====> Epoch: 2800 Average validation loss 15.24\n",
      "====> Epoch: 3000 Average training loss 14.47\n",
      "====> Epoch: 3000 Average validation loss 15.26\n"
     ]
    }
   ],
   "source": [
    "best_model_path_shallow_res = train(\n",
    "    {**shallow_params, 'use_shortcut': True},\n",
    "    'synthetic_data_shallow_vae_res',\n",
    "    trainloader, testloader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep network without residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 200 Average training loss 15.63\n",
      "====> Epoch: 200 Average validation loss 15.34\n",
      "====> Epoch: 400 Average training loss 14.68\n",
      "====> Epoch: 400 Average validation loss 14.34\n",
      "====> Epoch: 600 Average training loss 14.43\n",
      "====> Epoch: 600 Average validation loss 14.12\n",
      "====> Epoch: 800 Average training loss 14.15\n",
      "====> Epoch: 800 Average validation loss 13.92\n",
      "====> Epoch: 1000 Average training loss 14.00\n",
      "====> Epoch: 1000 Average validation loss 13.81\n",
      "====> Epoch: 1200 Average training loss 13.85\n",
      "====> Epoch: 1200 Average validation loss 13.84\n",
      "====> Epoch: 1400 Average training loss 13.81\n",
      "====> Epoch: 1400 Average validation loss 13.87\n",
      "====> Epoch: 1600 Average training loss 13.73\n",
      "====> Epoch: 1600 Average validation loss 13.88\n",
      "====> Epoch: 1800 Average training loss 13.67\n",
      "====> Epoch: 1800 Average validation loss 13.89\n",
      "====> Epoch: 2000 Average training loss 13.63\n",
      "====> Epoch: 2000 Average validation loss 13.97\n",
      "====> Epoch: 2200 Average training loss 13.58\n",
      "====> Epoch: 2200 Average validation loss 13.95\n",
      "====> Epoch: 2400 Average training loss 13.60\n",
      "====> Epoch: 2400 Average validation loss 13.94\n",
      "====> Epoch: 2600 Average training loss 13.58\n",
      "====> Epoch: 2600 Average validation loss 13.96\n",
      "====> Epoch: 2800 Average training loss 13.55\n",
      "====> Epoch: 2800 Average validation loss 13.99\n",
      "====> Epoch: 3000 Average training loss 13.56\n",
      "====> Epoch: 3000 Average validation loss 14.03\n"
     ]
    }
   ],
   "source": [
    "best_model_path_deep = train(\n",
    "    {**deep_params, 'use_shortcut': False},\n",
    "    'synthetic_data_deep_vae',\n",
    "    trainloader, testloader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep network with residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 200 Average training loss 14.68\n",
      "====> Epoch: 200 Average validation loss 13.47\n",
      "====> Epoch: 400 Average training loss 14.16\n",
      "====> Epoch: 400 Average validation loss 13.25\n",
      "====> Epoch: 600 Average training loss 13.94\n",
      "====> Epoch: 600 Average validation loss 13.09\n",
      "====> Epoch: 800 Average training loss 13.79\n",
      "====> Epoch: 800 Average validation loss 12.91\n",
      "====> Epoch: 1000 Average training loss 13.61\n",
      "====> Epoch: 1000 Average validation loss 12.94\n",
      "====> Epoch: 1200 Average training loss 13.50\n",
      "====> Epoch: 1200 Average validation loss 12.81\n",
      "====> Epoch: 1400 Average training loss 13.47\n",
      "====> Epoch: 1400 Average validation loss 12.95\n",
      "====> Epoch: 1600 Average training loss 13.44\n",
      "====> Epoch: 1600 Average validation loss 12.84\n",
      "====> Epoch: 1800 Average training loss 13.26\n",
      "====> Epoch: 1800 Average validation loss 12.79\n",
      "====> Epoch: 2000 Average training loss 13.27\n",
      "====> Epoch: 2000 Average validation loss 12.75\n",
      "====> Epoch: 2200 Average training loss 13.29\n",
      "====> Epoch: 2200 Average validation loss 12.95\n",
      "====> Epoch: 2400 Average training loss 13.22\n",
      "====> Epoch: 2400 Average validation loss 12.86\n",
      "====> Epoch: 2600 Average training loss 13.12\n",
      "====> Epoch: 2600 Average validation loss 12.82\n",
      "====> Epoch: 2800 Average training loss 13.15\n",
      "====> Epoch: 2800 Average validation loss 13.04\n",
      "====> Epoch: 3000 Average training loss 13.19\n",
      "====> Epoch: 3000 Average validation loss 12.88\n"
     ]
    }
   ],
   "source": [
    "best_model_path_deep_res = train(\n",
    "    {**deep_params, 'use_shortcut': True},\n",
    "    'synthetic_data_deep_vae_res',\n",
    "    trainloader, testloader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation and model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model run from previous section contains a saved model weights. In this section, we load these weights into their respective models and using the provided `generate_fake` routine, will generate `50_000` additional data points for class 1. We then evaluate the quality of these generated data using the provided test routines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With increased model capacity, `hidden_dims = [64, 32, 16]` and `latent_dim = 6`, the validation loss becomes smaller than the training loss. Since I don't understnad this behaviour, we will discard this model from further evaluation. We are left therefore with two models: \n",
    "- Vanilla VAE with hidden layers 50 and 12 and latent dimension of 3\n",
    "- VAE with residual blocks of the same dimensions as vanilla VAE. \n",
    "\n",
    "Next we read the best saved models of the two experiements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake(model, loader, scaler, no_samples, device):\n",
    "    mus = []\n",
    "    logvars = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            _, mu, logvar = model(data)\n",
    "            mus.append(mu)\n",
    "            logvars.append(logvar)\n",
    "    \n",
    "    mu = torch.cat(mus, dim=0)\n",
    "    logvar = torch.cat(logvars, dim=0)\n",
    "\n",
    "    sigma = torch.exp(logvar / 2)\n",
    "    q = torch.distributions.Normal(mu.mean(dim=0), sigma.mean(dim=0))\n",
    "    z = q.rsample(sample_shape=torch.Size([no_samples]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model.decoder(z).cpu().numpy()\n",
    "\n",
    "    fake_data = scaler.inverse_transform(pred)\n",
    "    return fake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the tests slightly by increasing the `max_iteration` to 1000 from 100 to avoid convegence issues with optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def run_test(x_df, y_df):\n",
    "    mlp = MLPClassifier(max_iter=1000)\n",
    "    ## Feel free to play with these parameters if you want\n",
    "    parameter_space = {\n",
    "        'hidden_layer_sizes': [(5,10), (12), (2,5,10, 15)],\n",
    "        'activation': ['tanh', 'relu', 'logistic'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05, 0.01],\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "\n",
    "    clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "    clf.fit(x_df, y_df)\n",
    "\n",
    "    print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "    #Compare actuals with predicted values\n",
    "    y_true, y_pred = y_df , clf.predict(x_df)\n",
    "\n",
    "    print('Results on the test set:')\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and mix with original data as requested by the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_params, model_path):\n",
    "    vae = model.VAE(**model_params)\n",
    "    vae.load_state_dict(torch.load(model_path)['model_state_dict'])    \n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_data_and_run_test(model_params, model_path, loader, scaler, num_samples, device, df_original):\n",
    "    model = load_model(model_params, model_path).to(device)\n",
    "    fake_data = generate_fake(model, loader, scaler, num_samples, device)\n",
    "    df_fake = pd.DataFrame(fake_data, columns=df_original.columns)\n",
    "    # set the 'Loan Status' to 1\n",
    "    df_fake['Loan Status'] = 1\n",
    "    df_augmented = pd.concat([df_original, df_fake], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    print('Running GridSearchCV with max iteration = 1000 ...')\n",
    "    run_test(*TestModel.load_xy(df_augmented))\n",
    "    return df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV with max iteration = 1000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (5, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "Results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.85     61222\n",
      "           1       0.82      0.88      0.85     56241\n",
      "\n",
      "    accuracy                           0.85    117463\n",
      "   macro avg       0.85      0.85      0.85    117463\n",
      "weighted avg       0.86      0.85      0.85    117463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aug_shallow = generate_fake_data_and_run_test(shallow_params, best_model_path_shallow, testloader, trainset.scaler, 50_000, device, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV with max iteration = 1000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (2, 5, 10, 15), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "Results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85     61222\n",
      "           1       0.87      0.78      0.82     56241\n",
      "\n",
      "    accuracy                           0.83    117463\n",
      "   macro avg       0.84      0.83      0.83    117463\n",
      "weighted avg       0.84      0.83      0.83    117463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aug_shallow_res = generate_fake_data_and_run_test({**shallow_params, 'use_shortcut': True},\n",
    "                                                    best_model_path_shallow_res,\n",
    "                                                    testloader, trainset.scaler, 50_000, device, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV with max iteration = 1000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': 12, 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "Results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.56      0.63     61222\n",
      "           1       0.62      0.76      0.68     56241\n",
      "\n",
      "    accuracy                           0.66    117463\n",
      "   macro avg       0.67      0.66      0.66    117463\n",
      "weighted avg       0.67      0.66      0.66    117463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aug_deep = generate_fake_data_and_run_test({**deep_params, 'use_shortcut': False},\n",
    "                                                best_model_path_deep,\n",
    "                                                testloader, trainset.scaler, 50_000, device, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV with max iteration = 1000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srn/Documents/code/venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'logistic', 'alpha': 0.05, 'hidden_layer_sizes': 12, 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "Results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.57      0.64     61222\n",
      "           1       0.62      0.77      0.69     56241\n",
      "\n",
      "    accuracy                           0.67    117463\n",
      "   macro avg       0.68      0.67      0.66    117463\n",
      "weighted avg       0.68      0.67      0.66    117463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aug_deep_res = generate_fake_data_and_run_test({**deep_params, 'use_shortcut': True},\n",
    "                                                    best_model_path_deep_res,\n",
    "                                                    testloader, trainset.scaler, 50_000, device, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In the work presented above, four networks were tested for generation of synthetic data:\n",
    "1. Shallow network without residual connections\n",
    "2. Shallow network with residual connections\n",
    "3. Deep network without residual connections\n",
    "4. Deep network with residual connections\n",
    "\n",
    "The following observations have been made:\n",
    "- The impact of residual connections on the quality of generated data, as indicated by F1 scores, has been inconclusive. For the shallow networks, the one without residual connections performed better, while for the deep network, the one with residual connections was slightly better.\n",
    "- When comparing deep and shallow networks, the latter significantly outperformed the former, as evidenced by much higher F1 scores.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
